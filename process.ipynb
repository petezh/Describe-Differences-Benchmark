{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.2.4)\n",
      "Requirement already satisfied: gdown in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (4.4.0)\n",
      "Requirement already satisfied: scipy in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.20.1)\n",
      "Collecting dload\n",
      "  Downloading dload-0.6-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: requests>=2.11.1 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from dload->-r requirements.txt (line 6)) (2.25.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.11.1->dload->-r requirements.txt (line 6)) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.11.1->dload->-r requirements.txt (line 6)) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.11.1->dload->-r requirements.txt (line 6)) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.11.1->dload->-r requirements.txt (line 6)) (4.0.0)\n",
      "Requirement already satisfied: tqdm in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 2)) (4.59.0)\n",
      "Requirement already satisfied: filelock in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 2)) (3.0.12)\n",
      "Requirement already satisfied: six in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 2)) (1.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from gdown->-r requirements.txt (line 2)) (4.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 1)) (2021.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 2)) (2.2.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from requests>=2.11.1->dload->-r requirements.txt (line 6)) (1.7.1)\n",
      "Installing collected packages: dload\n",
      "Successfully installed dload-0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore\n",
    "import requests\n",
    "import gdown\n",
    "from datetime import timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "  - https://github.com/sfu-discourse-lab/SOCC\n",
    "  - https://nextit-public.s3-us-west-2.amazonaws.com/rsics.html#all95data95by95thresholdcsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Deception\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her open-domain deception paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_f_l_1</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>There is a great deal of truth to the anti-vax...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_f_l_2</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Jenny mccarthy is a learned doctor who deserve...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_f_l_3</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Driving doesn't really require any practice.</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_f_l_4</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Drinking and driving is a winning and safe com...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_f_l_5</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Good hygiene isn't really important or attract...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id _gender  age         education country  \\\n",
       "0  1_f_l_1  Female   26  Bachelors degree  Canada   \n",
       "1  1_f_l_2  Female   26  Bachelors degree  Canada   \n",
       "2  1_f_l_3  Female   26  Bachelors degree  Canada   \n",
       "3  1_f_l_4  Female   26  Bachelors degree  Canada   \n",
       "4  1_f_l_5  Female   26  Bachelors degree  Canada   \n",
       "\n",
       "                                                text class  \n",
       "0  There is a great deal of truth to the anti-vax...   lie  \n",
       "1  Jenny mccarthy is a learned doctor who deserve...   lie  \n",
       "2       Driving doesn't really require any practice.   lie  \n",
       "3  Drinking and driving is a winning and safe com...   lie  \n",
       "4  Good hygiene isn't really important or attract...   lie  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/OpenDeception/7Truth7LiesDataset.csv'\n",
    "df = pd.read_csv(input_file, quotechar=\"'\", escapechar=\"\\\\\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['lie'] = list(df[df['class']=='lie']['text']) # positive denotes lie\n",
    "data['truth'] = list(df[df['class']=='truth']['text']) # negative denotes truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Lies and truths from any domain generated via crowdsourcing.'\n",
    "}\n",
    "output_file = 'output/open_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her fake news paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/fakeNewsDatasets/fakeNewsDataset/**/*.txt')\n",
    "legit, fake = [], []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if 'legit' in file:\n",
    "            legit.append(contents)\n",
    "        else:\n",
    "            fake.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['legit'] = legit\n",
    "data['fake'] = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Crowdsourced news articles from various domains.'\n",
    "}\n",
    "output_file = 'output/fake_news.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Deception\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her real life deception paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/RealLifeDeception/Transcription/**/*.txt')\n",
    "truth, lie = [], []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if 'truth' in file:\n",
    "            truth.extend(contents)\n",
    "        else:\n",
    "            lie.extend(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['truth'] = truth # lie\n",
    "data['lie'] = lie # truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Deception from criminal justice cases.'\n",
    "}\n",
    "output_file = 'output/real_life_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football\n",
    "\n",
    "Taken from Merullo et al.'s Football Commentary [dataset](https://arxiv.org/abs/1909.03343)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/football/football_15.json'\n",
    "with open(input_file, 'r') as f:\n",
    "    js_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "white, nonwhite = [], []\n",
    "for instance in js_data.values():\n",
    "    race = instance['label']['race']\n",
    "    commentary = ' '.join(instance['mention']).replace('<player>', 'the player')\n",
    "    if race == 'white':\n",
    "        white.append(commentary)\n",
    "    else:\n",
    "        nonwhite.append(commentary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['white'] = white # white\n",
    "data['nonwhite'] = nonwhite # nonwhite\n",
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Live football commentary on players by race.'\n",
    "}\n",
    "output_file = 'output/football.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Gender 1\n",
    "\n",
    "Pulled from preprocessed RateMyProfessor text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/profgender/full-data.txt'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['woman'] = list(df[df['Professor Gender']=='W']['Comment Text']) # woman\n",
    "data['man'] = list(df[df['Professor Gender']=='M']['Comment Text']) # man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'RateMyProfessor comments on male and female professors.'\n",
    "}\n",
    "output_file = 'output/prof_gender.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Gender 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "files = glob.glob('data/ratemyprof/*.csv')\n",
    "for file in files:\n",
    "    df = df.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "get_gender = lambda name: d.get_gender(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male             86699\n",
       "female           40619\n",
       "unknown          11741\n",
       "mostly_male       4724\n",
       "mostly_female     4639\n",
       "andy              1710\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_name'] = df['prof_name'].str.split().str[0]\n",
    "df['gender'] = df['first_name'].apply(get_gender)\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "df = df.sample(frac=1, random_state = 0)\n",
    "data['female'] = list(map(str, df[df['gender']=='female']['text'])) # woman\n",
    "data['male'] = list(map(str, df[df['gender']=='male']['text'])) # man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['female'] = [t for t in data['female'] if ' him ' not in t and ' his ' not in t]\n",
    "data['male'] = [t for t in data['male'] if 'she' not in t and ' her ' not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'RateMyProfessor comments on male and female professors.'\n",
    "}\n",
    "output_file = 'output/prof_gender.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenting Topics\n",
    "\n",
    "Read sentences pulled from various parenting topics from [Gao et al.](https://dl.acm.org/doi/10.1145/3411764.3445203)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So I participated in the survey re: exclusive ...</td>\n",
       "      <td>1</td>\n",
       "      <td>breastfeeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've started researching what pumps my insuran...</td>\n",
       "      <td>1</td>\n",
       "      <td>breastfeeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three and a half year old while listening to E...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>About a week ago, my 2 1/2 year old started co...</td>\n",
       "      <td>1</td>\n",
       "      <td>economy,child product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When is it positive to say your kid does not l...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  So I participated in the survey re: exclusive ...      1   \n",
       "1  I've started researching what pumps my insuran...      1   \n",
       "2  Three and a half year old while listening to E...      1   \n",
       "3  About a week ago, my 2 1/2 year old started co...      1   \n",
       "4  When is it positive to say your kid does not l...      1   \n",
       "\n",
       "                  topics  \n",
       "0          breastfeeding  \n",
       "1          breastfeeding  \n",
       "2                    NaN  \n",
       "3  economy,child product  \n",
       "4                    NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/parenting/0527_reddit_1300_parenting_clean.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "topics = set(itertools.chain.from_iterable(df['topics'].str.split(',')))\n",
    "data = {}\n",
    "for topic in topics:\n",
    "    data[topic] = list(df[df['topics'].str.contains(topic)]['text'].apply(lambda s: s.replace(u\"\\u2018\", \"'\").replace(u\"\\u2019\", \"'\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions on Reddit.'\n",
    "}\n",
    "output_file = 'output/parenting.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenting Users\n",
    "\n",
    "Using original source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'data/parenting/labeled_posts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GpYLez_wxJWiYQ3F6nDJLH7ymdaGsI87\n",
      "To: /Users/peterzhang/Documents/GitHub/diff_dist/data/parenting/labeled_posts.csv\n",
      "100%|██████████| 669M/669M [00:18<00:00, 36.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/parenting/labeled_posts.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = '1GpYLez_wxJWiYQ3F6nDJLH7ymdaGsI87'\n",
    "url = f'https://drive.google.com/uc?id={id}'\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0) # shuffle\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_post = df[['author', 'datetime']].groupby('author').min()\n",
    "get_first_post = dict(zip(first_post.index, first_post['datetime']))\n",
    "df['first_post'] = df['author'].apply(lambda x: get_first_post[x])\n",
    "df['age'] = df['datetime'] - df['first_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new          268421\n",
       "half year     83370\n",
       "90 days       73844\n",
       "30 days       72817\n",
       "1 year        57341\n",
       "2 years       23824\n",
       "3 years       12922\n",
       "4 years        7903\n",
       "5 years        1636\n",
       "Name: stage, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['new', '30 days', '90 days', 'half year', '1 year', '2 years', '3 years', '4 years', '5 years']\n",
    "days = (0, 30, 90, 180, 365, 2*365, 3*365, 4*365, 5*365, 100*365) # first 90 days, first year, first three years\n",
    "bins = [timedelta(days=d) for d in days]\n",
    "df['stage'] = pd.cut(df['age'], bins=bins, right=False, labels=labels)\n",
    "df['stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for stage in labels:\n",
    "    data[stage] = list(df[df['stage'] == stage]['selftext'])[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions at different stages of parenting.'\n",
    "}\n",
    "output_file = 'output/parenting_stages.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_subreddits = ('Mommit', 'NewMomStuff')\n",
    "dad_subreddits = ('daddit', 'NewDads')\n",
    "is_mom = df[['author', 'subreddit']].groupby('author').agg(lambda x: x.isin(mom_subreddits).any())\n",
    "is_dad = df[['author', 'subreddit']].groupby('author').agg(lambda x: x.isin(dad_subreddits).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_is_mom = dict(zip(is_mom.index, is_mom['subreddit']))\n",
    "get_is_dad = dict(zip(is_dad.index, is_dad['subreddit']))\n",
    "df['is_mom'] = df['author'].apply(lambda x: get_is_mom[x])\n",
    "df['is_dad'] = df['author'].apply(lambda x: get_is_dad[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['moms'] = list(df[df['is_mom'] & ~df['is_dad'] & ~df['subreddit'].isin(mom_subreddits)]['selftext'])\n",
    "data['dads'] = list(df[df['is_dad'] & ~df['is_mom'] & ~df['subreddit'].isin(dad_subreddits)]['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions by gender.'\n",
    "}\n",
    "output_file = 'output/parenting_genders.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['author'].value_counts()\n",
    "frequent_authors = list(counts.index[counts > 100])[2:] # exclude AutoModerator and [deleted]\n",
    "data = {}\n",
    "for author in frequent_authors:\n",
    "    data[author] = list(df[df['author'] == author]['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions by user.'\n",
    "}\n",
    "output_file = 'output/parenting_users.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization\n",
    "\n",
    "From Jerry Wei's news slant [dataset](https://github.com/JerryWei03/NewB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "lib_file = 'data/NewB-master/liberal.txt'\n",
    "with open(lib_file, 'r') as file:\n",
    "    data['lib'] = list(map(lambda x: x[2:], file.readlines()))\n",
    "con_file = 'data/NewB-master/conservative.txt'\n",
    "with open(con_file, 'r') as file:\n",
    "    data['con'] = list(map(lambda x: x[2:], file.readlines()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'News slant dataset on articles about Donald Trump.'\n",
    "}\n",
    "output_file = 'output/news_slant.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ukraine Bias\n",
    "\n",
    "Annotated data from [Farber et al.](https://github.com/michaelfaerber/ukraine-news-bias) on news coverage during the Ukraine crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia claims thousands fleeing Ukraine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia says 143,000 Ukrainians have already le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thousands of Ukrainians are fleeing across the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the border services, since the be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The head of the citizenship department of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0            Russia claims thousands fleeing Ukraine  0\n",
       "1  Russia says 143,000 Ukrainians have already le...  0\n",
       "2  Thousands of Ukrainians are fleeing across the...  0\n",
       "3  According to the border services, since the be...  0\n",
       "4  The head of the citizenship department of the ...  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/ukraine/sentences-with-binary-labels-bias.csv'\n",
    "df = pd.read_csv(input_file, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['bias'] = list(df[df[1]==1][0])\n",
    "data['no bias'] = list(df[df[1]==0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Recent articles about Ukraine labeled for subjectiveness.'\n",
    "}\n",
    "output_file = 'output/ukraine.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essays\n",
    "\n",
    "Automated essay scoring [data](https://www.kaggle.com/c/asap-aes/data?select=training_set_rel3.xlsx) from the Hewlett Foundaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/papers/training_set_rel3.tsv'\n",
    "df = pd.read_csv(input_file, sep='\\t', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=df.groupby('essay_set').domain1_score.transform('mean')    \n",
    "std=df.groupby('essay_set').domain1_score.transform('std')\n",
    "df['z_score'] = (df['domain1_score'] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['bad'] = list(df[df['z_score'] < 0]['essay'])\n",
    "data['good'] = list(df[0 <= df['z_score']]['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Set of essays answering 6 prompts with scores, divided into top and bottom half.'\n",
    "}\n",
    "output_file = 'output/essays.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diplomacy\n",
    "\n",
    "From the [Diplomacy project](https://sites.google.com/view/qanta/projects/diplomacy) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/diplomacy/*.txt')\n",
    "data = {}\n",
    "data['truth'] = []\n",
    "data['lie'] = []\n",
    "for file in files:\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    messages = list(itertools.chain.from_iterable(pd.read_json(files[0], lines=True)['messages']))\n",
    "    labels = list(itertools.chain.from_iterable(pd.read_json(files[0], lines=True)['sender_labels']))\n",
    "    df = pd.DataFrame({'message':messages, 'label':labels})\n",
    "    data['truth'].extend(list(df[df['label']==True]['message']))\n",
    "    data['lie'].extend(list(df[df['label']==False]['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Commentary from games of Diplomacy labeled for deceptiveness.'\n",
    "}\n",
    "output_file = 'output/diplomacy.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Headlines\n",
    "\n",
    "From the ABC \"million news headlines\" [dataset](https://www.kaggle.com/therohk/million-headlines), \"spam clickbait catalog\" [dataset](https://www.kaggle.com/therohk/examine-the-examiner), and India news headlines [dataset](https://www.kaggle.com/therohk/india-headlines-news-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_files = [\n",
    "    ('data/abc_headlines/abcnews-date-text.csv', 'output/abc_headlines.json', \"ABC\"),\n",
    "    ('data/examiner_headlines/examiner-date-text.csv', 'output/examiner_headlines.json', \"The Examminer\"),\n",
    "    ('data/india_headlines/india-news-headlines.csv', 'output/india_headlines.json', \"India news\")\n",
    "]\n",
    "for input_file, output_file, outlet in headline_files:\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['year'] = df['publish_date'].astype(str).str[:4].astype(int)\n",
    "    data = {}\n",
    "    for year in df['year'].unique():\n",
    "        data[str(year)] = list(df[df['year']==year]['headline_text'])\n",
    "    output = {\n",
    "        'distributions':data,\n",
    "        'note':'',\n",
    "        'description':f'Headlines from {outlet}'\n",
    "    }\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Relevance\n",
    "\n",
    "From the \"Home Depot Product Search Relevance\" [dataset](https://www.kaggle.com/c/home-depot-product-search-relevance/data?select=train.csv.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/search_relevance/train.csv'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['low relevance'] = list(df[df['relevance'] < 2.5]['search_term'])\n",
    "data['high relevance'] = list(df[df['relevance'] >= 2.5]['search_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Home Depot product searches labeled for the relevance of search results.'\n",
    "}\n",
    "output_file = 'output/search_relevance.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Reviews\n",
    "\n",
    "Reviews of products from Jianmo Ni's [Amazon Review Data](https://nijianmo.github.io/amazon/index.html#complete-data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/amazon_reviews/AMAZON_FASHION_5.json\n",
      "data/amazon_reviews/All_Beauty_5.json\n",
      "data/amazon_reviews/Appliances_5.json\n",
      "data/amazon_reviews/Arts_Crafts_and_Sewing_5.json\n",
      "data/amazon_reviews/Cell_Phones_and_Accessories_5.json\n",
      "data/amazon_reviews/Automotive_5.json\n"
     ]
    }
   ],
   "source": [
    "review_files = [\n",
    "    ('data/amazon_reviews/AMAZON_FASHION_5.json','output/amazon_fashion_reviews.json', 'fashion products'),\n",
    "    ('data/amazon_reviews/All_Beauty_5.json','output/beauty_reviews.json', 'beauty products'),\n",
    "    ('data/amazon_reviews/Appliances_5.json','output/appliances_reviews.json', 'appliances'),\n",
    "    ('data/amazon_reviews/Arts_Crafts_and_Sewing_5.json','output/arts_crafts_reviews.json', 'arts and crafts products'),\n",
    "    ('data/amazon_reviews/Cell_Phones_and_Accessories_5.json','output/phone_reviews.json', 'phones'),\n",
    "    ('data/amazon_reviews/Automotive_5.json','output/automotive_reviews.json', 'automative products')\n",
    "]\n",
    "for input_file, output_file, product in review_files:\n",
    "    print(input_file)\n",
    "    df = pd.read_json(input_file, lines=True)\n",
    "    data = {}\n",
    "    for rating in df['overall'].unique():\n",
    "        data[str(rating)] = list(df[df['overall'] == rating]['reviewText'].astype(str))\n",
    "    output = {\n",
    "        'distributions':data,\n",
    "        'note':'',\n",
    "        'description':f'Dataset of reviews of {product} on Amazon.'\n",
    "    }\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mafia Deception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/mafia/docs.pkl'\n",
    "df = pd.read_pickle(input_file, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['scum'] = list(df[df['scum']]['content'])[:1000]\n",
    "data['not scum'] = list(df[~df['scum']]['content'])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Commentary from games of Mafia labeled for deception.'\n",
    "}\n",
    "output_file = 'output/mafia_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks Reddit\n",
    "\n",
    "From [\"Daily News for Stock Market Prediction.\"](https://www.kaggle.com/datasets/aaron7sun/stocknews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'data/stocks_reddit/Combined_News_DJIA.csv'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['down'] = []\n",
    "data['up'] = []\n",
    "for col in df.columns:\n",
    "    if \"Top\" in col:\n",
    "        data['down'].extend(list(df[df['Label'] == 0][col]))\n",
    "        data['up'].extend(list(df[df['Label'] == 1][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Reddit headlines on days where markets rose vs fell.'\n",
    "}\n",
    "output_file = 'output/reddit_stocks.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unhealthy Conversations\n",
    "\n",
    "From the [Unhealthy Comments Corpus](https://github.com/conversationai/unhealthy-conversations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/unhealthy_convo/*.csv')\n",
    "data = defaultdict(list)\n",
    "attributes = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'generalisation_unfair', 'healthy', 'hostile', 'sarcastic']\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    for attr in attributes:\n",
    "        data[attr].extend(list(df[df[attr] == 1]['comment']))\n",
    "        data['not_' + attr].extend(list(df[df[attr] == 0]['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Unhealthy comments scraped from Reddit labeled by attribute.'\n",
    "}\n",
    "output_file = 'output/unhealthy_convo.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Posts\n",
    "\n",
    "From the [2012-2016 Facebook Posts](https://data.world/martinchek/2012-2016-facebook-posts/workspace) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagenames_file = 'data/facebook_posts/fb_news_pagenames.csv'\n",
    "pagenames_df = pd.read_csv(pagenames_file)\n",
    "pagenames_dict = dict(zip(pagenames_df['page_id'], pagenames_df['page_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>message</th>\n",
       "      <th>page_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>react_angry</th>\n",
       "      <th>react_haha</th>\n",
       "      <th>react_like</th>\n",
       "      <th>react_love</th>\n",
       "      <th>react_sad</th>\n",
       "      <th>react_wow</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>shares</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-14T14:30:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>We are #LIVE outside the National Rifle Associ...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890879532217</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>993</td>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>139</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-14T14:20:59+0000</td>\n",
       "      <td></td>\n",
       "      <td>http://bbc.in/2talMsx</td>\n",
       "      <td>UPDATE: \\r\\n-2 Ukrainian tourists killed in st...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890968202217</td>\n",
       "      <td>172</td>\n",
       "      <td>8</td>\n",
       "      <td>994</td>\n",
       "      <td>11</td>\n",
       "      <td>783</td>\n",
       "      <td>264</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>680</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-14T13:40:38+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>Proms: Come with us on a tour of the Royal Alb...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890852247217</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2034</td>\n",
       "      <td>369</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>395</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-14T12:55:45+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/142678...</td>\n",
       "      <td>Thousands say their final goodbyes to Bradley ...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_1426789250735491</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2262</td>\n",
       "      <td>754</td>\n",
       "      <td>1989</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>542</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-14T12:45:00+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>Despite safety warnings, this beach near an ai...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890645702217</td>\n",
       "      <td>65</td>\n",
       "      <td>513</td>\n",
       "      <td>4336</td>\n",
       "      <td>54</td>\n",
       "      <td>128</td>\n",
       "      <td>815</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>1956</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time description  \\\n",
       "0  2017-07-14T14:30:59+0000         NaN   \n",
       "1  2017-07-14T14:20:59+0000               \n",
       "2  2017-07-14T13:40:38+0000         NaN   \n",
       "3  2017-07-14T12:55:45+0000         NaN   \n",
       "4  2017-07-14T12:45:00+0000         NaN   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "1                              http://bbc.in/2talMsx   \n",
       "2  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "3  https://www.facebook.com/bbcnews/videos/142678...   \n",
       "4  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "\n",
       "                                             message       page_id  \\\n",
       "0  We are #LIVE outside the National Rifle Associ...  228735667216   \n",
       "1  UPDATE: \\r\\n-2 Ukrainian tourists killed in st...  228735667216   \n",
       "2  Proms: Come with us on a tour of the Royal Alb...  228735667216   \n",
       "3  Thousands say their final goodbyes to Bradley ...  228735667216   \n",
       "4  Despite safety warnings, this beach near an ai...  228735667216   \n",
       "\n",
       "                          post_id  react_angry  react_haha  react_like  \\\n",
       "0  228735667216_10154890879532217           54          24         993   \n",
       "1  228735667216_10154890968202217          172           8         994   \n",
       "2  228735667216_10154890852247217            5          12        2034   \n",
       "3   228735667216_1426789250735491            6           0        2262   \n",
       "4  228735667216_10154890645702217           65         513        4336   \n",
       "\n",
       "   react_love  react_sad  react_wow                 scrape_time  shares  \\\n",
       "0         144         12         24  2017-07-14 11:01:24.379857     139   \n",
       "1          11        783        264  2017-07-14 11:01:24.379857     680   \n",
       "2         369          6         45  2017-07-14 11:01:24.379857     395   \n",
       "3         754       1989         11  2017-07-14 11:01:24.379857     542   \n",
       "4          54        128        815  2017-07-14 11:01:24.379857    1956   \n",
       "\n",
       "  page_name  \n",
       "0       bbc  \n",
       "1       bbc  \n",
       "2       bbc  \n",
       "3       bbc  \n",
       "4       bbc  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = 'data/facebook_posts/fb_news_posts_20K.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df['page_name'] = df['page_id'].apply(lambda x: pagenames_dict[x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for page in pagenames_dict.values():\n",
    "    data[page] = list(df[df['page_name'] == page]['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Dataset of Facebook posts from pages of varoius news outlets.'\n",
    "}\n",
    "output_file = 'output/fb_posts.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews\n",
    "\n",
    "Read subset of Yelp reviews. Add unziped [Yelp dataset](https://www.yelp.com/dataset/documentation/main) to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_file = 'data/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "business_file = 'data/yelp_dataset/yelp_academic_dataset_business.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(reviews_file, lines=True, nrows=100000)\n",
    "business_df = pd.read_json(business_file, lines=True, nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore potential restaurant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.merge(business_df[['business_id', 'categories']], on='business_id', how='left')\n",
    "reviews_df = reviews_df[reviews_df['categories'].apply(lambda x: bool(x) and 'Restaurants' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_types = {\n",
    "    'Pizza':'pizza',\n",
    "    'Mexican':'mexican',\n",
    "    'Chinese':'chinese',\n",
    "    'Italian':'italian',\n",
    "    'American (New)':'new_american',\n",
    "    'American (Traditional)':'trad_american',\n",
    "    'Thai':'thai',\n",
    "    'Vietnamese':'vietnamese',\n",
    "    'Seafood':'seafood',\n",
    "    'Barbeque':'bbq',\n",
    "    'Diners':'diner',\n",
    "    'Japanese':'japanese'\n",
    "}\n",
    "for category, keyword in restaurant_types.items():\n",
    "    reviews_df['is_' + keyword] = reviews_df['categories'].apply(lambda x: bool(x) and category in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for keyword in restaurant_types.values():\n",
    "    data[keyword] = list(reviews_df[reviews_df['is_' + keyword]]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Reviews of Yelp restaurants categorized by restaurant type.'\n",
    "}\n",
    "output_file = 'output/yelp_reviews.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/reddit-dataset/*.csv')\n",
    "header = ['text', 'id', 'subreddit', 'meta', 'time', 'author', 'ups', 'downs', 'authorlinkkarma', 'authorcommentkarma', 'authorisgold']\n",
    "reddit_df = pd.DataFrame()\n",
    "malformed =  ('anime', 'comicbooks', 'movies', 'harrypotter')\n",
    "for file in files:\n",
    "    sub_df = pd.read_csv(file)\n",
    "    if any([sub in file for sub in malformed]):\n",
    "        sub_df = sub_df.iloc[1:, 1:]\n",
    "    sub_df = sub_df.drop(sub_df.columns[0], axis=1)\n",
    "    sub_df = sub_df.rename(dict(zip(sub_df.columns, header)), axis=1)\n",
    "    reddit_df = reddit_df.append(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['datetime'] = pd.to_datetime(reddit_df['time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = {}\n",
    "for sub in reddit_df['subreddit'].unique():\n",
    "    sub_df = reddit_df[reddit_df.subreddit == sub]\n",
    "    days = (max(sub_df.time) - min(sub_df.time))/3600/24\n",
    "    sub_data[sub] = list(sub_df['text'])[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':sub_data,\n",
    "    'note':'',\n",
    "    'description':'Reddit comments from various subreddits.'\n",
    "}\n",
    "output_file = 'output/reddit_posts.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('data/C50/**/**/*.txt')\n",
    "data = defaultdict(list)\n",
    "for file in files:\n",
    "    author = file.split('/')[3]\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        data[author].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Texts by various Rueters authors on similar topics.'\n",
    "}\n",
    "output_file = 'output/rueters_authors.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization & Tweets\n",
    "\n",
    "Sentiment140 dataset with [1.6M Tweets](https://www.kaggle.com/datasets/kazanova/sentiment140)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/covid_tweets/training.csv'\n",
    "df = pd.read_csv(path, names=['id', 'timstamp', 'type', 'user', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_pairs = [\n",
    "    ('your', [' ur '], [' your ', ' you\\'re ']),\n",
    "    ('with', [' wit '], [' with ']),\n",
    "    ('that', [' dat ', ' dats '], [' that ']),   \n",
    "    ('going', [' goin '], ['going ']),\n",
    "    ('know', [' kno '], [' know ']),\n",
    "    ('you', [' u '], [' you ']),\n",
    "    ('what', [' wut ', ' wat '], [' what ']),\n",
    "    ('the', [' da '], [' the '])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for group, misspell, proper in normalization_pairs:\n",
    "    data[group + '_misspell'] = list(df[df['text'].str.contains('|'.join(misspell))]['text'])\n",
    "    data[group + '_proper'] = list(df[df['text'].str.contains('|'.join(proper))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Tweets with and without mispellings of common words..'\n",
    "}\n",
    "output_file = 'output/tweet_misspellings.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
