{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from scipy.stats import zscore\n",
    "import requests\n",
    "import gdown\n",
    "from datetime import timedelta\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data'\n",
    "output_folder = '../output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Deception\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her open-domain deception paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>_gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_f_l_1</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>There is a great deal of truth to the anti-vax...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_f_l_2</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Jenny mccarthy is a learned doctor who deserve...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_f_l_3</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Driving doesn't really require any practice.</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_f_l_4</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Drinking and driving is a winning and safe com...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_f_l_5</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>Bachelors degree</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Good hygiene isn't really important or attract...</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id _gender  age         education country  \\\n",
       "0  1_f_l_1  Female   26  Bachelors degree  Canada   \n",
       "1  1_f_l_2  Female   26  Bachelors degree  Canada   \n",
       "2  1_f_l_3  Female   26  Bachelors degree  Canada   \n",
       "3  1_f_l_4  Female   26  Bachelors degree  Canada   \n",
       "4  1_f_l_5  Female   26  Bachelors degree  Canada   \n",
       "\n",
       "                                                text class  \n",
       "0  There is a great deal of truth to the anti-vax...   lie  \n",
       "1  Jenny mccarthy is a learned doctor who deserve...   lie  \n",
       "2       Driving doesn't really require any practice.   lie  \n",
       "3  Drinking and driving is a winning and safe com...   lie  \n",
       "4  Good hygiene isn't really important or attract...   lie  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = f'{data_folder}/OpenDeception/7Truth7LiesDataset.csv'\n",
    "df = pd.read_csv(input_file, quotechar=\"'\", escapechar=\"\\\\\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['lie'] = list(df[df['class']=='lie']['text']) # positive denotes lie\n",
    "data['truth'] = list(df[df['class']=='truth']['text']) # negative denotes truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Lies and truths from any domain generated via crowdsourcing.'\n",
    "}\n",
    "output_file = f'{output_folder}/open_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her fake news paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/fakeNewsDatasets/fakeNewsDataset/**/*.txt')\n",
    "legit, fake = [], []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if 'legit' in file:\n",
    "            legit.append(contents)\n",
    "        else:\n",
    "            fake.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['legit'] = legit\n",
    "data['fake'] = fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Crowdsourced news articles from various domains.'\n",
    "}\n",
    "output_file = f'{output_folder}/fake_news.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Deception\n",
    "\n",
    "Downloaded from Rada Mihalcea's [website](https://web.eecs.umich.edu/~mihalcea/downloads.html), based on her real life deception paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/RealLifeDeception/Transcription/**/*.txt')\n",
    "truth, lie = [], []\n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        contents = f.read()\n",
    "        if 'truth' in file:\n",
    "            truth.append(contents)\n",
    "        else:\n",
    "            lie.append(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['truth'] = truth # lie\n",
    "data['lie'] = lie # truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Deception from criminal justice cases.'\n",
    "}\n",
    "output_file = f'{output_folder}/real_life_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Football\n",
    "\n",
    "Taken from Merullo et al.'s Football Commentary [dataset](https://arxiv.org/abs/1909.03343)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/football/football_15.json'\n",
    "with open(input_file, 'r') as f:\n",
    "    js_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "white, nonwhite = [], []\n",
    "for instance in js_data.values():\n",
    "    race = instance['label']['race']\n",
    "    commentary = ' '.join(instance['mention']).replace('<player>', 'the player')\n",
    "    if race == 'white':\n",
    "        white.append(commentary)\n",
    "    else:\n",
    "        nonwhite.append(commentary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['white'] = white # white\n",
    "data['nonwhite'] = nonwhite # nonwhite\n",
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Live football commentary on players by race.'\n",
    "}\n",
    "output_file = f'{output_folder}/football.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Gender 1\n",
    "\n",
    "Pulled from preprocessed RateMyProfessor text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/profgender/full-data.txt'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['woman'] = list(df[df['Professor Gender']=='W']['Comment Text']) # woman\n",
    "data['man'] = list(df[df['Professor Gender']=='M']['Comment Text']) # man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'RateMyProfessor comments on male and female professors.'\n",
    "}\n",
    "output_file = f'{output_folder}/prof_gender.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Professor Gender 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "files = glob.glob(f'{data_folder}/ratemyprof/*.csv')\n",
    "for file in files:\n",
    "    df = df.append(pd.read_csv(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gender_guesser.detector as gender\n",
    "d = gender.Detector()\n",
    "get_gender = lambda name: d.get_gender(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "male             86699\n",
       "female           40619\n",
       "unknown          11741\n",
       "mostly_male       4724\n",
       "mostly_female     4639\n",
       "andy              1710\n",
       "Name: gender, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['first_name'] = df['prof_name'].str.split().str[0]\n",
    "df['gender'] = df['first_name'].apply(get_gender)\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "df = df.sample(frac=1, random_state = 0)\n",
    "data['female'] = list(map(str, df[df['gender']=='female']['text'])) # woman\n",
    "data['male'] = list(map(str, df[df['gender']=='male']['text'])) # man"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['female'] = [t for t in data['female'] if ' him ' not in t and ' his ' not in t]\n",
    "data['male'] = [t for t in data['male'] if 'she' not in t and ' her ' not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'RateMyProfessor comments on male and female professors.'\n",
    "}\n",
    "output_file = f'{output_folder}/prof_gender.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenting Topics\n",
    "\n",
    "Read sentences pulled from various parenting topics from [Gao et al.](https://dl.acm.org/doi/10.1145/3411764.3445203)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So I participated in the survey re: exclusive ...</td>\n",
       "      <td>1</td>\n",
       "      <td>breastfeeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I've started researching what pumps my insuran...</td>\n",
       "      <td>1</td>\n",
       "      <td>breastfeeding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three and a half year old while listening to E...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>About a week ago, my 2 1/2 year old started co...</td>\n",
       "      <td>1</td>\n",
       "      <td>economy,child product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When is it positive to say your kid does not l...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  \\\n",
       "0  So I participated in the survey re: exclusive ...      1   \n",
       "1  I've started researching what pumps my insuran...      1   \n",
       "2  Three and a half year old while listening to E...      1   \n",
       "3  About a week ago, my 2 1/2 year old started co...      1   \n",
       "4  When is it positive to say your kid does not l...      1   \n",
       "\n",
       "                  topics  \n",
       "0          breastfeeding  \n",
       "1          breastfeeding  \n",
       "2                    NaN  \n",
       "3  economy,child product  \n",
       "4                    NaN  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = f'{data_folder}/parenting/0527_reddit_1300_parenting_clean.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "topics = set(itertools.chain.from_iterable(df['topics'].str.split(',')))\n",
    "data = {}\n",
    "for topic in topics:\n",
    "    data[topic] = list(df[df['topics'].str.contains(topic)]['text'].apply(lambda s: s.replace(u\"\\u2018\", \"'\").replace(u\"\\u2019\", \"'\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions on Reddit.'\n",
    "}\n",
    "output_file = f'{output_folder}/parenting.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parenting Users\n",
    "\n",
    "Using original source data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = f'{data_folder}/parenting/labeled_posts.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GpYLez_wxJWiYQ3F6nDJLH7ymdaGsI87\n",
      "To: /Users/peterzhang/Documents/GitHub/diff_dist/data/parenting/labeled_posts.csv\n",
      "100%|██████████| 669M/669M [00:18<00:00, 36.8MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'data/parenting/labeled_posts.csv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = '1GpYLez_wxJWiYQ3F6nDJLH7ymdaGsI87'\n",
    "url = f'https://drive.google.com/uc?id={id}'\n",
    "gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=0) # shuffle\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_post = df[['author', 'datetime']].groupby('author').min()\n",
    "get_first_post = dict(zip(first_post.index, first_post['datetime']))\n",
    "df['first_post'] = df['author'].apply(lambda x: get_first_post[x])\n",
    "df['age'] = df['datetime'] - df['first_post']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new          268421\n",
       "half year     83370\n",
       "90 days       73844\n",
       "30 days       72817\n",
       "1 year        57341\n",
       "2 years       23824\n",
       "3 years       12922\n",
       "4 years        7903\n",
       "5 years        1636\n",
       "Name: stage, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['new', '30 days', '90 days', 'half year', '1 year', '2 years', '3 years', '4 years', '5 years']\n",
    "days = (0, 30, 90, 180, 365, 2*365, 3*365, 4*365, 5*365, 100*365) # first 90 days, first year, first three years\n",
    "bins = [timedelta(days=d) for d in days]\n",
    "df['stage'] = pd.cut(df['age'], bins=bins, right=False, labels=labels)\n",
    "df['stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for stage in labels:\n",
    "    data[stage] = list(df[df['stage'] == stage]['selftext'])[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions at different stages of parenting.'\n",
    "}\n",
    "output_file = f'{output_folder}/parenting_stages.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_subreddits = ('Mommit', 'NewMomStuff')\n",
    "dad_subreddits = ('daddit', 'NewDads')\n",
    "is_mom = df[['author', 'subreddit']].groupby('author').agg(lambda x: x.isin(mom_subreddits).any())\n",
    "is_dad = df[['author', 'subreddit']].groupby('author').agg(lambda x: x.isin(dad_subreddits).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_is_mom = dict(zip(is_mom.index, is_mom['subreddit']))\n",
    "get_is_dad = dict(zip(is_dad.index, is_dad['subreddit']))\n",
    "df['is_mom'] = df['author'].apply(lambda x: get_is_mom[x])\n",
    "df['is_dad'] = df['author'].apply(lambda x: get_is_dad[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['moms'] = list(df[df['is_mom'] & ~df['is_dad'] & ~df['subreddit'].isin(mom_subreddits)]['selftext'])\n",
    "data['dads'] = list(df[df['is_dad'] & ~df['is_mom'] & ~df['subreddit'].isin(dad_subreddits)]['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions by gender.'\n",
    "}\n",
    "output_file = f'{output_folder}/parenting_genders.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['author'].value_counts()\n",
    "frequent_authors = list(counts.index[counts > 100])[2:] # exclude AutoModerator and [deleted]\n",
    "data = {}\n",
    "for author in frequent_authors:\n",
    "    data[author] = list(df[df['author'] == author]['selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Parenting discussions by user.'\n",
    "}\n",
    "output_file = f'{output_folder}/parenting_users.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polarization\n",
    "\n",
    "From Jerry Wei's news slant [dataset](https://github.com/JerryWei03/NewB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "lib_file = f'{data_folder}/NewB-master/liberal.txt'\n",
    "with open(lib_file, 'r') as file:\n",
    "    data['lib'] = list(map(lambda x: x[2:], file.readlines()))\n",
    "con_file = f'{data_folder}/NewB-master/conservative.txt'\n",
    "with open(con_file, 'r') as file:\n",
    "    data['con'] = list(map(lambda x: x[2:], file.readlines()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'News slant dataset on articles about Donald Trump.'\n",
    "}\n",
    "output_file = f'{output_folder}/news_slant.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ukraine Bias\n",
    "\n",
    "Annotated data from [Farber et al.](https://github.com/michaelfaerber/ukraine-news-bias) on news coverage during the Ukraine crisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Russia claims thousands fleeing Ukraine</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia says 143,000 Ukrainians have already le...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thousands of Ukrainians are fleeing across the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>According to the border services, since the be...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The head of the citizenship department of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0            Russia claims thousands fleeing Ukraine  0\n",
       "1  Russia says 143,000 Ukrainians have already le...  0\n",
       "2  Thousands of Ukrainians are fleeing across the...  0\n",
       "3  According to the border services, since the be...  0\n",
       "4  The head of the citizenship department of the ...  0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = f'{data_folder}/ukraine/sentences-with-binary-labels-bias.csv'\n",
    "df = pd.read_csv(input_file, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['bias'] = list(df[df[1]==1][0])\n",
    "data['no bias'] = list(df[df[1]==0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Recent articles about Ukraine labeled for subjectiveness.'\n",
    "}\n",
    "output_file = f'{output_folder}/ukraine.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Essays\n",
    "\n",
    "Automated essay scoring [data](https://www.kaggle.com/c/asap-aes/data?select=training_set_rel3.xlsx) from the Hewlett Foundaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/papers/training_set_rel3.tsv'\n",
    "df = pd.read_csv(input_file, sep='\\t', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    649\n",
       "3    572\n",
       "1    302\n",
       "4    258\n",
       "0     24\n",
       "Name: domain1_score, dtype: int64"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['essay_set'] == 5].domain1_score.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean=df.groupby('essay_set').domain1_score.transform('mean')    \n",
    "std=df.groupby('essay_set').domain1_score.transform('std')\n",
    "df['z_score'] = (df['domain1_score'] - mean)/std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['bad'] = list(df[df['z_score'] < 0]['essay'])\n",
    "data['good'] = list(df[0 <= df['z_score']]['essay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Set of essays answering 6 prompts with scores, divided into top and bottom half.'\n",
    "}\n",
    "output_file = f'{output_folder}/essays.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diplomacy\n",
    "\n",
    "From the [Diplomacy project](https://sites.google.com/view/qanta/projects/diplomacy) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/diplomacy/*.json')\n",
    "data = {}\n",
    "data['truth'] = []\n",
    "data['lie'] = []\n",
    "for file in files:\n",
    "    df = pd.read_json(file, lines=True)\n",
    "    messages = list(itertools.chain.from_iterable(pd.read_json(files[0], lines=True)['messages']))\n",
    "    labels = list(itertools.chain.from_iterable(pd.read_json(files[0], lines=True)['sender_labels']))\n",
    "    df = pd.DataFrame({'message':messages, 'label':labels})\n",
    "    data['truth'].extend(list(df[df['label']==True]['message']))\n",
    "    data['lie'].extend(list(df[df['label']==False]['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Commentary from games of Diplomacy labeled for deceptiveness.'\n",
    "}\n",
    "output_file = f'{output_folder}/diplomacy.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Headlines\n",
    "\n",
    "From the ABC \"million news headlines\" [dataset](https://www.kaggle.com/therohk/million-headlines), \"spam clickbait catalog\" [dataset](https://www.kaggle.com/therohk/examine-the-examiner), and India news headlines [dataset](https://www.kaggle.com/therohk/india-headlines-news-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_files = [\n",
    "    (f'{data_folder}/abc_headlines/abcnews-date-text.csv', f'{output_folder}/abc_headlines.json', \"ABC\"),\n",
    "    (f'{data_folder}/examiner_headlines/examiner-date-text.csv', f'{output_folder}/examiner_headlines.json', \"The Examminer\"),\n",
    "    (f'{data_folder}/india_headlines/india-news-headlines.csv', f'{output_folder}/india_headlines.json', \"India news\")\n",
    "]\n",
    "for input_file, output_file, outlet in headline_files:\n",
    "    df = pd.read_csv(input_file)\n",
    "    df['year'] = df['publish_date'].astype(str).str[:4].astype(int)\n",
    "    data = {}\n",
    "    for year in df['year'].unique():\n",
    "        data[str(year)] = list(df[df['year']==year]['headline_text'])[:1000]\n",
    "    output = {\n",
    "        'distributions':data,\n",
    "        'note':'',\n",
    "        'description':f'Headlines from {outlet}'\n",
    "    }\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Relevance\n",
    "\n",
    "From the \"Home Depot Product Search Relevance\" [dataset](https://www.kaggle.com/c/home-depot-product-search-relevance/data?select=train.csv.zip)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/search_relevance/train.csv'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['low relevance'] = list(df[df['relevance'] < 2.5]['search_term'])\n",
    "data['high relevance'] = list(df[df['relevance'] >= 2.5]['search_term'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Home Depot product searches labeled for the relevance of search results.'\n",
    "}\n",
    "output_file = f'{output_folder}/search_relevance.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Reviews\n",
    "\n",
    "Reviews of products from Jianmo Ni's [Amazon Review Data](https://nijianmo.github.io/amazon/index.html#complete-data) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/amazon_reviews/AMAZON_FASHION_5.json\n",
      "data/amazon_reviews/All_Beauty_5.json\n",
      "data/amazon_reviews/Appliances_5.json\n",
      "data/amazon_reviews/Arts_Crafts_and_Sewing_5.json\n",
      "data/amazon_reviews/Cell_Phones_and_Accessories_5.json\n",
      "data/amazon_reviews/Automotive_5.json\n"
     ]
    }
   ],
   "source": [
    "review_files = [\n",
    "    (f'{data_folder}/amazon_reviews/AMAZON_FASHION_5.json',f'{output_folder}/amazon_fashion_reviews.json', 'fashion products'),\n",
    "    (f'{data_folder}/amazon_reviews/All_Beauty_5.json',f'{output_folder}/beauty_reviews.json', 'beauty products'),\n",
    "    (f'{data_folder}/amazon_reviews/Appliances_5.json',f'{output_folder}/appliances_reviews.json', 'appliances'),\n",
    "    (f'{data_folder}/amazon_reviews/Arts_Crafts_and_Sewing_5.json',f'{output_folder}/arts_crafts_reviews.json', 'arts and crafts products'),\n",
    "    (f'{data_folder}/amazon_reviews/Cell_Phones_and_Accessories_5.json',f'{output_folder}/phone_reviews.json', 'phones'),\n",
    "    (f'{data_folder}/amazon_reviews/Automotive_5.json',f'{output_folder}/automotive_reviews.json', 'automative products')\n",
    "]\n",
    "for input_file, output_file, product in review_files:\n",
    "    print(input_file)\n",
    "    df = pd.read_json(input_file, lines=True)\n",
    "    data = {}\n",
    "    for rating in df['overall'].unique():\n",
    "        data[str(rating)] = list(df[df['overall'] == rating]['reviewText'].astype(str))\n",
    "    output = {\n",
    "        'distributions':data,\n",
    "        'note':'',\n",
    "        'description':f'Dataset of reviews of {product} on Amazon.'\n",
    "    }\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mafia Deception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/mafia/docs.pkl'\n",
    "df = pd.read_pickle(input_file, 'gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['scum'] = list(df[df['scum']]['content'])[:1000]\n",
    "data['not scum'] = list(df[~df['scum']]['content'])[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Commentary from games of Mafia labeled for deception.'\n",
    "}\n",
    "output_file = f'{output_folder}/mafia_deception.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stocks Reddit\n",
    "\n",
    "From [\"Daily News for Stock Market Prediction.\"](https://www.kaggle.com/datasets/aaron7sun/stocknews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f'{data_folder}/stocks_reddit/Combined_News_DJIA.csv'\n",
    "df = pd.read_csv(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['down'] = []\n",
    "data['up'] = []\n",
    "for col in df.columns:\n",
    "    if \"Top\" in col:\n",
    "        data['down'].extend(list(df[df['Label'] == 0][col]))\n",
    "        data['up'].extend(list(df[df['Label'] == 1][col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Reddit headlines on days where markets rose vs fell.'\n",
    "}\n",
    "output_file = f'{output_folder}/reddit_stocks.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unhealthy Conversations\n",
    "\n",
    "From the [Unhealthy Comments Corpus](https://github.com/conversationai/unhealthy-conversations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/unhealthy_convo/*.csv')\n",
    "data = defaultdict(list)\n",
    "attributes = ['antagonize', 'condescending', 'dismissive', 'generalisation', 'generalisation_unfair', 'healthy', 'hostile', 'sarcastic']\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    for attr in attributes:\n",
    "        data[attr].extend(list(df[df[attr] == 1]['comment']))\n",
    "        data['not_' + attr].extend(list(df[df[attr] == 0]['comment']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Unhealthy comments scraped from Reddit labeled by attribute.'\n",
    "}\n",
    "output_file = f'{output_folder}/unhealthy_convo.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facebook Posts\n",
    "\n",
    "From the [2012-2016 Facebook Posts](https://data.world/martinchek/2012-2016-facebook-posts/workspace) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagenames_file = f'{data_folder}/facebook_posts/fb_news_pagenames.csv'\n",
    "pagenames_df = pd.read_csv(pagenames_file)\n",
    "pagenames_dict = dict(zip(pagenames_df['page_id'], pagenames_df['page_name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_time</th>\n",
       "      <th>description</th>\n",
       "      <th>link</th>\n",
       "      <th>message</th>\n",
       "      <th>page_id</th>\n",
       "      <th>post_id</th>\n",
       "      <th>react_angry</th>\n",
       "      <th>react_haha</th>\n",
       "      <th>react_like</th>\n",
       "      <th>react_love</th>\n",
       "      <th>react_sad</th>\n",
       "      <th>react_wow</th>\n",
       "      <th>scrape_time</th>\n",
       "      <th>shares</th>\n",
       "      <th>page_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-07-14T14:30:59+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>We are #LIVE outside the National Rifle Associ...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890879532217</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>993</td>\n",
       "      <td>144</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>139</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-07-14T14:20:59+0000</td>\n",
       "      <td></td>\n",
       "      <td>http://bbc.in/2talMsx</td>\n",
       "      <td>UPDATE: \\r\\n-2 Ukrainian tourists killed in st...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890968202217</td>\n",
       "      <td>172</td>\n",
       "      <td>8</td>\n",
       "      <td>994</td>\n",
       "      <td>11</td>\n",
       "      <td>783</td>\n",
       "      <td>264</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>680</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-07-14T13:40:38+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>Proms: Come with us on a tour of the Royal Alb...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890852247217</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>2034</td>\n",
       "      <td>369</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>395</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-07-14T12:55:45+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/142678...</td>\n",
       "      <td>Thousands say their final goodbyes to Bradley ...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_1426789250735491</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2262</td>\n",
       "      <td>754</td>\n",
       "      <td>1989</td>\n",
       "      <td>11</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>542</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-07-14T12:45:00+0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.facebook.com/bbcnews/videos/101548...</td>\n",
       "      <td>Despite safety warnings, this beach near an ai...</td>\n",
       "      <td>228735667216</td>\n",
       "      <td>228735667216_10154890645702217</td>\n",
       "      <td>65</td>\n",
       "      <td>513</td>\n",
       "      <td>4336</td>\n",
       "      <td>54</td>\n",
       "      <td>128</td>\n",
       "      <td>815</td>\n",
       "      <td>2017-07-14 11:01:24.379857</td>\n",
       "      <td>1956</td>\n",
       "      <td>bbc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_time description  \\\n",
       "0  2017-07-14T14:30:59+0000         NaN   \n",
       "1  2017-07-14T14:20:59+0000               \n",
       "2  2017-07-14T13:40:38+0000         NaN   \n",
       "3  2017-07-14T12:55:45+0000         NaN   \n",
       "4  2017-07-14T12:45:00+0000         NaN   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "1                              http://bbc.in/2talMsx   \n",
       "2  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "3  https://www.facebook.com/bbcnews/videos/142678...   \n",
       "4  https://www.facebook.com/bbcnews/videos/101548...   \n",
       "\n",
       "                                             message       page_id  \\\n",
       "0  We are #LIVE outside the National Rifle Associ...  228735667216   \n",
       "1  UPDATE: \\r\\n-2 Ukrainian tourists killed in st...  228735667216   \n",
       "2  Proms: Come with us on a tour of the Royal Alb...  228735667216   \n",
       "3  Thousands say their final goodbyes to Bradley ...  228735667216   \n",
       "4  Despite safety warnings, this beach near an ai...  228735667216   \n",
       "\n",
       "                          post_id  react_angry  react_haha  react_like  \\\n",
       "0  228735667216_10154890879532217           54          24         993   \n",
       "1  228735667216_10154890968202217          172           8         994   \n",
       "2  228735667216_10154890852247217            5          12        2034   \n",
       "3   228735667216_1426789250735491            6           0        2262   \n",
       "4  228735667216_10154890645702217           65         513        4336   \n",
       "\n",
       "   react_love  react_sad  react_wow                 scrape_time  shares  \\\n",
       "0         144         12         24  2017-07-14 11:01:24.379857     139   \n",
       "1          11        783        264  2017-07-14 11:01:24.379857     680   \n",
       "2         369          6         45  2017-07-14 11:01:24.379857     395   \n",
       "3         754       1989         11  2017-07-14 11:01:24.379857     542   \n",
       "4          54        128        815  2017-07-14 11:01:24.379857    1956   \n",
       "\n",
       "  page_name  \n",
       "0       bbc  \n",
       "1       bbc  \n",
       "2       bbc  \n",
       "3       bbc  \n",
       "4       bbc  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = f'{data_folder}/facebook_posts/fb_news_posts_20K.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df['page_name'] = df['page_id'].apply(lambda x: pagenames_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for page in pagenames_dict.values():\n",
    "    data[page] = list(df[df['page_name'] == page]['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Dataset of Facebook posts from pages of varoius news outlets.'\n",
    "}\n",
    "output_file = f'{output_folder}/fb_posts.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yelp Reviews\n",
    "\n",
    "Read subset of Yelp reviews. Add unziped [Yelp dataset](https://www.yelp.com/dataset/documentation/main) to the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_file = f'{data_folder}/yelp_dataset/yelp_academic_dataset_review.json'\n",
    "business_file = f'{data_folder}/yelp_dataset/yelp_academic_dataset_business.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = pd.read_json(reviews_file, lines=True, nrows=100000)\n",
    "business_df = pd.read_json(business_file, lines=True, nrows=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore potential restaurant types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.merge(business_df[['business_id', 'categories']], on='business_id', how='left')\n",
    "reviews_df = reviews_df[reviews_df['categories'].apply(lambda x: bool(x) and 'Restaurants' in x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_types = {\n",
    "    'Pizza':'pizza',\n",
    "    'Mexican':'mexican',\n",
    "    'Chinese':'chinese',\n",
    "    'Italian':'italian',\n",
    "    'American (New)':'new_american',\n",
    "    'American (Traditional)':'trad_american',\n",
    "    'Thai':'thai',\n",
    "    'Vietnamese':'vietnamese',\n",
    "    'Seafood':'seafood',\n",
    "    'Barbeque':'bbq',\n",
    "    'Diners':'diner',\n",
    "    'Japanese':'japanese'\n",
    "}\n",
    "for category, keyword in restaurant_types.items():\n",
    "    reviews_df['is_' + keyword] = reviews_df['categories'].apply(lambda x: bool(x) and category in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for keyword in restaurant_types.values():\n",
    "    data[keyword] = list(reviews_df[reviews_df['is_' + keyword]]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Reviews of Yelp restaurants categorized by restaurant type.'\n",
    "}\n",
    "output_file = f'{output_folder}/yelp_reviews.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/reddit-dataset/*.csv')\n",
    "header = ['text', 'id', 'subreddit', 'meta', 'time', 'author', 'ups', 'downs', 'authorlinkkarma', 'authorcommentkarma', 'authorisgold']\n",
    "reddit_df = pd.DataFrame()\n",
    "malformed =  ('anime', 'comicbooks', 'movies', 'harrypotter')\n",
    "for file in files:\n",
    "    sub_df = pd.read_csv(file)\n",
    "    if any([sub in file for sub in malformed]):\n",
    "        sub_df = sub_df.iloc[1:, 1:]\n",
    "    sub_df = sub_df.drop(sub_df.columns[0], axis=1)\n",
    "    sub_df = sub_df.rename(dict(zip(sub_df.columns, header)), axis=1)\n",
    "    reddit_df = reddit_df.append(sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df['datetime'] = pd.to_datetime(reddit_df['time'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data = {}\n",
    "for sub in reddit_df['subreddit'].unique():\n",
    "    sub_df = reddit_df[reddit_df.subreddit == sub]\n",
    "    days = (max(sub_df.time) - min(sub_df.time))/3600/24\n",
    "    sub_data[sub] = list(sub_df['text'])[:8000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':sub_data,\n",
    "    'note':'',\n",
    "    'description':'Reddit comments from various subreddits.'\n",
    "}\n",
    "output_file = f'{output_folder}/reddit_posts.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authorship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f'{data_folder}/C50/**/**/*.txt')\n",
    "data = defaultdict(list)\n",
    "for file in files:\n",
    "    author = file.split('/')[3]\n",
    "    with open(file, 'r') as f:\n",
    "        text = f.read()\n",
    "        data[author].append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Texts by various Reuters authors on similar topics.'\n",
    "}\n",
    "output_file = f'{output_folder}/reuters_authors.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspelling\n",
    "\n",
    "Sentiment140 dataset with [1.6M Tweets](https://www.kaggle.com/datasets/kazanova/sentiment140)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{data_folder}/covid_tweets/training.csv'\n",
    "df = pd.read_csv(path, names=['id', 'timstamp', 'type', 'user', 'text'], encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_pairs = [\n",
    "    ('your', [' ur '], [' your ', ' you\\'re ']),\n",
    "    ('with', [' wit '], [' with ']),\n",
    "    ('that', [' dat ', ' dats '], [' that ']),   \n",
    "    ('going', [' goin '], ['going ']),\n",
    "    ('know', [' kno '], [' know ']),\n",
    "    ('you', [' u '], [' you ']),\n",
    "    ('what', [' wut ', ' wat '], [' what ']),\n",
    "    ('the', [' da '], [' the '])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for group, misspell, proper in normalization_pairs:\n",
    "    data[group + '_misspell'] = list(df[df['text'].str.contains('|'.join(misspell))]['text'])\n",
    "    data[group + '_proper'] = list(df[df['text'].str.contains('|'.join(proper))]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Tweets with and without mispellings of common words.'\n",
    "}\n",
    "output_file = f'{output_folder}/tweet_misspellings.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Humor\n",
    "\n",
    "From the [Reddit Humor Detection](https://github.com/orionw/RedditHumorDetection) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['dev.tsv', 'train.tsv']\n",
    "df = pd.DataFrame()\n",
    "for f in files:\n",
    "    path = f'{data_folder}/reddithumor/{f}'\n",
    "    df = df.append(pd.read_csv(path, names=['index', 'funny', 'type', 'text'], encoding='latin-1'))\n",
    "df = df.drop(['index', 'type'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "def process(text):\n",
    "    return text.replace('_____', ' ')\n",
    "data['funny'] = list(df[df['funny'] == 1]['text'].apply(process))\n",
    "data['unfunny'] = list(df[df['funny'] == 0]['text'].apply(process))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Funny and unfunny Reddit jokes, as judged by proportion of upvotes.'\n",
    "}\n",
    "output_file = f'{output_folder}/reddit_jokes.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ECHR\n",
    "\n",
    "The [ECHR dataset](https://archive.org/details/ECHR-ACL2019)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '{data_folder}/ECHR_Dataset/*_Anon/*.json'\n",
    "files = glob.glob(path)\n",
    "dicts = [json.load(open(f, 'r')) for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "data = defaultdict(list)\n",
    "for d in np.random.permutation(dicts)[:1000]:\n",
    "    text = '\\n'.join(d['TEXT'])\n",
    "    if d['VIOLATED_ARTICLES']:\n",
    "        data['violation'].append(text)\n",
    "    else:\n",
    "        data['no_violation'].append(text)\n",
    "data = dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Court decisions of the ECHR, categorized by whether there was a violation.'\n",
    "}\n",
    "output_file = f'{output_folder}/echr_decisions.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCOTUS Decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{data_folder}/scotus/task1_data.pkl'\n",
    "df = pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'Facts': 'facts'}, inplace=True)\n",
    "df.drop(columns=['index'], inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mirror = df.copy()\n",
    "df_mirror[['second_party', 'first_party']] = df_mirror[['first_party', 'second_party']]\n",
    "df = df.append(df_mirror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['first_party'] + ' ' + df['facts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['win'] = df['first_party'] == df['winning_party']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['win'] = list(df[df['win']]['text'])\n",
    "data['lose'] = list(df[~df['win']]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Winners and losers of SCOTUS cases with facts, mirrored from each party\\s perspective.'\n",
    "}\n",
    "output_file = f'{output_folder}/scotus_decisions.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{data_folder}/news_pop/News_Final.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_bottom(group, col, n = 50):\n",
    "    sorted = group.sort_values(col)\n",
    "    return sorted.iloc[-n:], sorted.iloc[:n]\n",
    "\n",
    "def clean_text(text):\n",
    "    return str(text).replace('&quot;', '\"').replace('\"\"\"', '\"')\n",
    "\n",
    "df['Headline'] = df['Headline'].apply(clean_text)\n",
    "df['Title'] = df['Title'].apply(clean_text)\n",
    "\n",
    "def rank_sentiment(group):\n",
    "    return top_bottom(group, 'SentimentTitle')\n",
    "\n",
    "def rank_fb(group):\n",
    "    return top_bottom(group, 'Facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_groups = df.groupby('Topic').apply(rank_sentiment)\n",
    "data = {}\n",
    "for topic in sentiment_groups.keys():\n",
    "    pos, neg = sentiment_groups[topic]\n",
    "    data[topic + '_pos'] = list(pos['Title'])\n",
    "    data[topic + '_neg'] = list(neg['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb_df = df[(df.Source == 'Bloomberg') & (df.Facebook >= 0) & (df.Topic.isin(['obama', 'economy', 'microsoft']))]\n",
    "fb_groups = fb_df.groupby('Topic').apply(rank_fb)\n",
    "for topic in fb_groups.keys():\n",
    "    pop, unpop = fb_groups[topic]\n",
    "    data[topic + '_pop'] = list(pop['Title'])\n",
    "    data[topic + '_unpop'] = list(unpop['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Headlines on four topics with social feedback on four platforms.'\n",
    "}\n",
    "output_file = f'{output_folder}/news_pop.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convincingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = f'{data_folder}/convincingness/*'\n",
    "files = glob.glob(data_path)\n",
    "df = pd.DataFrame(columns=['id', 'rank', 'argument'])\n",
    "def read_file(file):\n",
    "    f = open(file, 'r')\n",
    "    lines = f.readlines()\n",
    "    data = []\n",
    "    for line in lines[1:]:\n",
    "        id, rank, argument = line.split('\\t')\n",
    "        data.append([id, rank, argument])\n",
    "    return pd.DataFrame(data, columns=['id', 'rank', 'argument'])\n",
    "for file in files:\n",
    "    df = df.append(read_file(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_bottom(group, col, n = 50):\n",
    "    sorted = group.sort_values(col)\n",
    "    return sorted.iloc[-n:], sorted.iloc[:n]\n",
    "\n",
    "data = {}\n",
    "unconvincing, convincing = top_bottom(df, 'rank', 500)\n",
    "data['unconvincing'] = unconvincing\n",
    "data['convincing'] = convincing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'distributions':data,\n",
    "    'note':'',\n",
    "    'description':'Arguments on online forums ranked by convincingness.'\n",
    "}\n",
    "output_file = f'{output_folder}/convincingness.json'\n",
    "with open(output_file, 'w') as outfile:\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openreview\n",
    "client = openreview.Client(baseurl='https://api.openreview.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "invites = [\n",
    "    (2018, 'ICLR.cc/2018/Conference/-/Blind_Submission','ICLR.cc/2018/Conference/-/Paper.*/Official_Review'),\n",
    "    (2019, 'ICLR.cc/2019/Conference/-/Blind_Submission','ICLR.cc/2019/Conference/-/Paper.*/Official_Review'),\n",
    "    (2020, 'ICLR.cc/2020/Conference/-/Blind_Submission','ICLR.cc/2020/Conference/Paper.*/-/Official_Review'),\n",
    "    (2021, 'ICLR.cc/2021/Conference/-/Blind_Submission','ICLR.cc/2021/Conference/Paper.*/-/Official_Review'),\n",
    "]\n",
    "\n",
    "metadata = []\n",
    "\n",
    "for year, submission_invite, review_invite in invites:\n",
    "\n",
    "    submissions = openreview.tools.iterget_notes(\n",
    "        client, invitation=submission_invite)\n",
    "    submissions_by_forum = {n.forum: n for n in submissions}\n",
    "\n",
    "    reviews = openreview.tools.iterget_notes(\n",
    "        client, invitation=review_invite)\n",
    "    reviews_by_forum = defaultdict(list)\n",
    "    for review in reviews:\n",
    "        reviews_by_forum[review.forum].append(review)\n",
    "\n",
    "    for forum in submissions_by_forum:\n",
    "\n",
    "        forum_reviews = reviews_by_forum[forum]\n",
    "        review_ratings = [int(n.content['rating'][0]) for n in forum_reviews]\n",
    "        average_rating = np.mean(review_ratings)\n",
    "\n",
    "        submission_content = submissions_by_forum[forum].content\n",
    "        abstract = submission_content['abstract']\n",
    "\n",
    "        forum_metadata = {\n",
    "            'forum': forum,\n",
    "            'review_ratings': review_ratings,\n",
    "            'average_rating': average_rating,\n",
    "            'abstract': abstract,\n",
    "            'year': year,\n",
    "        }\n",
    "        metadata.append(forum_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forum</th>\n",
       "      <th>review_ratings</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>abstract</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ryBnUWb0b</td>\n",
       "      <td>[7, 6, 6]</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>In cities with tall buildings, emergency respo...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Skk3Jm96W</td>\n",
       "      <td>[7, 4, 6]</td>\n",
       "      <td>5.666667</td>\n",
       "      <td>We consider the problem of exploration in meta...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r1RQdCg0W</td>\n",
       "      <td>[6, 6, 6]</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>We present Merged-Averaged Classifiers via Has...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rJ3fy0k0Z</td>\n",
       "      <td>[6, 5, 5]</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>The goal of imitation learning (IL) is to enab...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SkBYYyZRZ</td>\n",
       "      <td>[5, 4, 7]</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>The choice of activation functions in deep net...</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7133</th>\n",
       "      <td>Dh29CAlnMW</td>\n",
       "      <td>[4, 4, 6]</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>The Automunge open source python library platf...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7134</th>\n",
       "      <td>Ig-VyQc-MLK</td>\n",
       "      <td>[6, 7, 4, 9]</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>Recent work has explored the possibility of pr...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7135</th>\n",
       "      <td>EXkD6ZjvJQQ</td>\n",
       "      <td>[6, 6, 7]</td>\n",
       "      <td>6.333333</td>\n",
       "      <td>This paper investigates the finite-sample pred...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7136</th>\n",
       "      <td>QjINdYOfq0b</td>\n",
       "      <td>[6, 4, 6]</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>We present Automatic Bit Sharing (ABS) to auto...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137</th>\n",
       "      <td>POWv6hDd9XH</td>\n",
       "      <td>[7, 8, 6, 7]</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>We study the challenging task of neural networ...</td>\n",
       "      <td>2021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7138 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            forum review_ratings  average_rating  \\\n",
       "0       ryBnUWb0b      [7, 6, 6]        6.333333   \n",
       "1       Skk3Jm96W      [7, 4, 6]        5.666667   \n",
       "2       r1RQdCg0W      [6, 6, 6]        6.000000   \n",
       "3       rJ3fy0k0Z      [6, 5, 5]        5.333333   \n",
       "4       SkBYYyZRZ      [5, 4, 7]        5.333333   \n",
       "...           ...            ...             ...   \n",
       "7133   Dh29CAlnMW      [4, 4, 6]        4.666667   \n",
       "7134  Ig-VyQc-MLK   [6, 7, 4, 9]        6.500000   \n",
       "7135  EXkD6ZjvJQQ      [6, 6, 7]        6.333333   \n",
       "7136  QjINdYOfq0b      [6, 4, 6]        5.333333   \n",
       "7137  POWv6hDd9XH   [7, 8, 6, 7]        7.000000   \n",
       "\n",
       "                                               abstract  year  \n",
       "0     In cities with tall buildings, emergency respo...  2018  \n",
       "1     We consider the problem of exploration in meta...  2018  \n",
       "2     We present Merged-Averaged Classifiers via Has...  2018  \n",
       "3     The goal of imitation learning (IL) is to enab...  2018  \n",
       "4     The choice of activation functions in deep net...  2018  \n",
       "...                                                 ...   ...  \n",
       "7133  The Automunge open source python library platf...  2021  \n",
       "7134  Recent work has explored the possibility of pr...  2021  \n",
       "7135  This paper investigates the finite-sample pred...  2021  \n",
       "7136  We present Automatic Bit Sharing (ABS) to auto...  2021  \n",
       "7137  We study the challenging task of neural networ...  2021  \n",
       "\n",
       "[7138 rows x 5 columns]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.average_rating >= 5].abstract.tolist()\n",
    "df[df.average_rating < 5].abstract.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py:311: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Expected 1 fields in line 6326, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-246-539f34de590c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://github.com/dbbrandt/short_answer_granding_capstone_project/blob/master/data/source_data/ShortAnswerGrading_v2.0/data/raw/all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'1232'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1045\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1047\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, rows)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m         \u001b[0malldata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rows_to_cols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclude_implicit_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malldata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_rows_to_cols\u001b[0;34m(self, content)\u001b[0m\n\u001b[1;32m    979\u001b[0m                     \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\". \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mreason\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_alert_malformed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# see gh-13320\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/io/parsers/python_parser.py\u001b[0m in \u001b[0;36m_alert_malformed\u001b[0;34m(self, msg, row_num)\u001b[0m\n\u001b[1;32m    720\u001b[0m         \"\"\"\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_bad_lines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadLineHandleMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mParserError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_bad_lines\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadLineHandleMethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWARN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Skipping line {row_num}: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Expected 1 fields in line 6326, saw 3. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "pd.read_csv('https://github.com/dbbrandt/short_answer_granding_capstone_project/blob/master/data/source_data/ShortAnswerGrading_v2.0/data/raw/all',sep='1232')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>_golden</th>\n",
       "      <th>_unit_state</th>\n",
       "      <th>_trusted_judgments</th>\n",
       "      <th>_last_judgment_at</th>\n",
       "      <th>gender</th>\n",
       "      <th>gender:confidence</th>\n",
       "      <th>profile_yn</th>\n",
       "      <th>profile_yn:confidence</th>\n",
       "      <th>created</th>\n",
       "      <th>...</th>\n",
       "      <th>profileimage</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>sidebar_color</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_count</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>815719226</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:24</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>12/5/13 1:48</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/414342229...</td>\n",
       "      <td>0</td>\n",
       "      <td>FFFFFF</td>\n",
       "      <td>Robbie E Responds To Critics After Win Against...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110964</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>main; @Kan1shk3</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>815719227</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:30</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10/1/12 13:51</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/539604221...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>ÛÏIt felt like they were my friends and I was...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7471</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>815719228</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:33</td>\n",
       "      <td>male</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/28/14 11:30</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657330418...</td>\n",
       "      <td>1</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>i absolutely adore when louis starts the songs...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5617</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>clcncl</td>\n",
       "      <td>Belgrade</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>815719229</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/26/15 23:10</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6/11/09 22:39</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/259703936...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>Hi @JordanSpieth - Looking at the url - do you...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1693</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Palo Alto, CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>815719230</td>\n",
       "      <td>False</td>\n",
       "      <td>finalized</td>\n",
       "      <td>3</td>\n",
       "      <td>10/27/15 1:15</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/16/14 13:23</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/564094871...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Watching Neighbours on Sky+ catching up with t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31462</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20045</th>\n",
       "      <td>815757572</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>259</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8/5/15 21:16</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/656793310...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>@lookupondeath ...Fine, and I'll drink tea too...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>783</td>\n",
       "      <td>10/26/15 13:20</td>\n",
       "      <td>6.587400e+17</td>\n",
       "      <td>Verona ªÁ</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20046</th>\n",
       "      <td>815757681</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>248</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8/15/12 21:17</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/639815429...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Greg Hardy you a good player and all but don't...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13523</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Kansas City, MO</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20047</th>\n",
       "      <td>815757830</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9/3/12 1:17</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/655473271...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>You can miss people and still never want to se...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26419</td>\n",
       "      <td>10/26/15 13:20</td>\n",
       "      <td>6.587400e+17</td>\n",
       "      <td>Lagos Nigeria</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20048</th>\n",
       "      <td>815757921</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>0.8489</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11/6/12 23:46</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/657716093...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@bitemyapp i had noticed your tendency to pee ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56073</td>\n",
       "      <td>10/26/15 12:40</td>\n",
       "      <td>6.587300e+17</td>\n",
       "      <td>Texas Hill Country</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20049</th>\n",
       "      <td>815757985</td>\n",
       "      <td>True</td>\n",
       "      <td>golden</td>\n",
       "      <td>249</td>\n",
       "      <td>NaN</td>\n",
       "      <td>female</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4/14/14 17:22</td>\n",
       "      <td>...</td>\n",
       "      <td>https://pbs.twimg.com/profile_images/655134724...</td>\n",
       "      <td>0</td>\n",
       "      <td>C0DEED</td>\n",
       "      <td>I think for my APUSH creative project I'm goin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2922</td>\n",
       "      <td>10/26/15 13:19</td>\n",
       "      <td>6.587400e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20050 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        _unit_id  _golden _unit_state  _trusted_judgments _last_judgment_at  \\\n",
       "0      815719226    False   finalized                   3    10/26/15 23:24   \n",
       "1      815719227    False   finalized                   3    10/26/15 23:30   \n",
       "2      815719228    False   finalized                   3    10/26/15 23:33   \n",
       "3      815719229    False   finalized                   3    10/26/15 23:10   \n",
       "4      815719230    False   finalized                   3     10/27/15 1:15   \n",
       "...          ...      ...         ...                 ...               ...   \n",
       "20045  815757572     True      golden                 259               NaN   \n",
       "20046  815757681     True      golden                 248               NaN   \n",
       "20047  815757830     True      golden                 264               NaN   \n",
       "20048  815757921     True      golden                 250               NaN   \n",
       "20049  815757985     True      golden                 249               NaN   \n",
       "\n",
       "       gender  gender:confidence profile_yn  profile_yn:confidence  \\\n",
       "0        male             1.0000        yes                    1.0   \n",
       "1        male             1.0000        yes                    1.0   \n",
       "2        male             0.6625        yes                    1.0   \n",
       "3        male             1.0000        yes                    1.0   \n",
       "4      female             1.0000        yes                    1.0   \n",
       "...       ...                ...        ...                    ...   \n",
       "20045  female             1.0000        yes                    1.0   \n",
       "20046    male             1.0000        yes                    1.0   \n",
       "20047    male             1.0000        yes                    1.0   \n",
       "20048  female             0.8489        yes                    1.0   \n",
       "20049  female             1.0000        yes                    1.0   \n",
       "\n",
       "              created  ...                                       profileimage  \\\n",
       "0        12/5/13 1:48  ...  https://pbs.twimg.com/profile_images/414342229...   \n",
       "1       10/1/12 13:51  ...  https://pbs.twimg.com/profile_images/539604221...   \n",
       "2      11/28/14 11:30  ...  https://pbs.twimg.com/profile_images/657330418...   \n",
       "3       6/11/09 22:39  ...  https://pbs.twimg.com/profile_images/259703936...   \n",
       "4       4/16/14 13:23  ...  https://pbs.twimg.com/profile_images/564094871...   \n",
       "...               ...  ...                                                ...   \n",
       "20045    8/5/15 21:16  ...  https://pbs.twimg.com/profile_images/656793310...   \n",
       "20046   8/15/12 21:17  ...  https://pbs.twimg.com/profile_images/639815429...   \n",
       "20047     9/3/12 1:17  ...  https://pbs.twimg.com/profile_images/655473271...   \n",
       "20048   11/6/12 23:46  ...  https://pbs.twimg.com/profile_images/657716093...   \n",
       "20049   4/14/14 17:22  ...  https://pbs.twimg.com/profile_images/655134724...   \n",
       "\n",
       "       retweet_count sidebar_color  \\\n",
       "0                  0        FFFFFF   \n",
       "1                  0        C0DEED   \n",
       "2                  1        C0DEED   \n",
       "3                  0        C0DEED   \n",
       "4                  0             0   \n",
       "...              ...           ...   \n",
       "20045              0        C0DEED   \n",
       "20046              0             0   \n",
       "20047              0        C0DEED   \n",
       "20048              0             0   \n",
       "20049              0        C0DEED   \n",
       "\n",
       "                                                    text tweet_coord  \\\n",
       "0      Robbie E Responds To Critics After Win Against...         NaN   \n",
       "1      ÛÏIt felt like they were my friends and I was...         NaN   \n",
       "2      i absolutely adore when louis starts the songs...         NaN   \n",
       "3      Hi @JordanSpieth - Looking at the url - do you...         NaN   \n",
       "4      Watching Neighbours on Sky+ catching up with t...         NaN   \n",
       "...                                                  ...         ...   \n",
       "20045  @lookupondeath ...Fine, and I'll drink tea too...         NaN   \n",
       "20046  Greg Hardy you a good player and all but don't...         NaN   \n",
       "20047  You can miss people and still never want to se...         NaN   \n",
       "20048  @bitemyapp i had noticed your tendency to pee ...         NaN   \n",
       "20049  I think for my APUSH creative project I'm goin...         NaN   \n",
       "\n",
       "      tweet_count   tweet_created      tweet_id      tweet_location  \\\n",
       "0          110964  10/26/15 12:40  6.587300e+17     main; @Kan1shk3   \n",
       "1            7471  10/26/15 12:40  6.587300e+17                 NaN   \n",
       "2            5617  10/26/15 12:40  6.587300e+17              clcncl   \n",
       "3            1693  10/26/15 12:40  6.587300e+17       Palo Alto, CA   \n",
       "4           31462  10/26/15 12:40  6.587300e+17                 NaN   \n",
       "...           ...             ...           ...                 ...   \n",
       "20045         783  10/26/15 13:20  6.587400e+17          Verona ªÁ   \n",
       "20046       13523  10/26/15 12:40  6.587300e+17     Kansas City, MO   \n",
       "20047       26419  10/26/15 13:20  6.587400e+17      Lagos Nigeria    \n",
       "20048       56073  10/26/15 12:40  6.587300e+17  Texas Hill Country   \n",
       "20049        2922  10/26/15 13:19  6.587400e+17                 NaN   \n",
       "\n",
       "                    user_timezone  \n",
       "0                         Chennai  \n",
       "1      Eastern Time (US & Canada)  \n",
       "2                        Belgrade  \n",
       "3      Pacific Time (US & Canada)  \n",
       "4                             NaN  \n",
       "...                           ...  \n",
       "20045                         NaN  \n",
       "20046                         NaN  \n",
       "20047                         NaN  \n",
       "20048                         NaN  \n",
       "20049                         NaN  \n",
       "\n",
       "[20050 rows x 26 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyreadr\n",
      "  Downloading pyreadr-0.4.4-cp38-cp38-macosx_10_9_x86_64.whl (250 kB)\n",
      "\u001b[K     |████████████████████████████████| 250 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2.0 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pyreadr) (1.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyreadr) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyreadr) (1.21.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from pandas>=1.2.0->pyreadr) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/peterzhang/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyreadr) (1.16.0)\n",
      "Installing collected packages: pyreadr\n",
      "Successfully installed pyreadr-0.4.4\n"
     ]
    }
   ],
   "source": [
    "!pip install pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "LibrdataError",
     "evalue": "Invalid file, or file has unsupported features",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLibrdataError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-c6756a785e39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyreadr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_r\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'corpus_nytimes_summary.Rds'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyreadr/pyreadr.py\u001b[0m in \u001b[0;36mread_r\u001b[0;34m(path, use_objects, timezone)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mPyreadrError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File {0} does not exist!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyreadr/librdata.pyx\u001b[0m in \u001b[0;36mpyreadr.librdata.Parser.parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pyreadr/librdata.pyx\u001b[0m in \u001b[0;36mpyreadr.librdata.Parser.parse\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mLibrdataError\u001b[0m: Invalid file, or file has unsupported features"
     ]
    }
   ],
   "source": [
    "result = pyreadr.read_r('corpus_nytimes_summary.Rds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../manual/BarnumLoNPTReplication/data/docs_by_committee/**/*.txt')\n",
    "docs = []\n",
    "for file in files:\n",
    "    year = re.findall('\\d\\d\\d\\d', file)[0]\n",
    "    with open(file, 'r', encoding='latin1') as f:\n",
    "        text = \" \".join(f.readlines())\n",
    "    doc = {\n",
    "        'year':int(year),\n",
    "        'text':text\n",
    "    }\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(docs)\n",
    "pre_2008 = df[df.year < 2008].text.tolist()\n",
    "btw_2008_2012 = df[(df.year >= 2008) & (df.year < 2012)].text.tolist()\n",
    "post_2012 = df[df.year >= 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adea540253e8e8f708a1ef36fc3af2830fee8642e0041a914a62b471ba922451"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
